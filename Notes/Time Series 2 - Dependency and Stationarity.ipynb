{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency\n",
    "\n",
    "Suppose we observe a time series $\\{x_t\\}$ at times $t_1, \\ldots, t_n$. The distribution of the observed data $x_{t_1}, \\ldots, x_{t_n}$ is characterized by the joint distribution function:\n",
    "\n",
    "$\n",
    "F_{t_1, \\ldots, t_n}(c_1, \\ldots, c_n) = P(x_{t_1} \\leq c_1, \\ldots, x_{t_n} \\leq c_n)\n",
    "$\n",
    "\n",
    "Usually, this is difficult to work with. We may consider the marginal distribution at time $t$:\n",
    "\n",
    "$\n",
    "F_t(x) = P(x_t \\leq x)\n",
    "$\n",
    "\n",
    "or the marginal density:\n",
    "\n",
    "$\n",
    "f_t(x) = \\frac{\\partial F_t(x)}{\\partial x}\n",
    "$\n",
    "\n",
    "assuming this quantity exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Mean Function\n",
    "\n",
    "The **mean function** of a time series is:\n",
    "\n",
    "$\n",
    "\\mu_{x_t} = E(x_t)\n",
    "$\n",
    "\n",
    "assuming this expectation exists. Note that the mean is a function of $ t $. When there is no ambiguity about which time series we are referring to, we may instead write $\\mu_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.14 Mean Function of a Moving Average Series\n",
    "\n",
    "$w_t$ denotes a white noise series.\n",
    "- $\\mu_{wt}=E(w_t)=0$\n",
    "\n",
    "$\\mu_{vt}=E(v_t)=\\frac{1}{3}[E(w_{t-1})+E(w_{t})+E(w_{t+1})]=0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.15 Mean Function of a Random Walk with Drift\n",
    "\n",
    "$x_{t} = \\delta\\,t + \\sum_{j=1}^{t}w_{j}, \\qquad t=1,2,\\ldots$\n",
    "- $E(w_t)=0$ for all $t$, and $\\delta$ is a constant.\n",
    "\n",
    "$\\mu_{xt} = \\mathrm{E}(x_{t}) = \\delta\\,t + \\sum_{j=1}^{t}\\mathrm{E}(w_{j}) = \\delta\\,t$\n",
    "\n",
    "which is a straight line with slope $\\delta$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.16: Mean Function of Signal Plus Noise\n",
    "\n",
    "$\\begin{aligned}\n",
    "\\mu_{xt} = \\mathrm{E}(x_{t}) \n",
    "&= \\mathrm{E}\\left[2\\cos\\left(2\\pi\\frac{t+15}{50}\\right) + w_{t}\\right] \\\\\n",
    "&= 2\\cos\\left(2\\pi\\frac{t+15}{50}\\right) + \\mathrm{E}(w_{t}) \\\\\n",
    "&= 2\\cos\\left(2\\pi\\frac{t+15}{50}\\right)\n",
    "\\end{aligned}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocovariance Function\n",
    "\n",
    "**Definition 1.2**: The autocovariance function is defined as the product moment\n",
    "\n",
    "$\n",
    "\\gamma_{x}(s,t) = \\operatorname{cov}(x_{s}, x_{t}) = \\operatorname{E}[(x_{s} - \\mu_{s})(x_{t} - \\mu_{t})], \\quad (1.10)\n",
    "$\n",
    "\n",
    "for all $s$ and $t$. When no possible confusion exists about which time series we are referring to, we will drop the subscript and write $\\gamma_{x}(s,t)$ as $\\gamma(s,t)$. Note that $\\gamma_{x}(s,t) = \\gamma_{x}(t,s)$ for all time points $s$ and $t$.\n",
    "\n",
    "- The autocovariance function measures linear dependence in time.\n",
    "- If $x_s$ and $x_t$ are jointly Normally distribution, $\\gamma(s,t)=0$ implies independence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.17 Autocovariance of White Noise\n",
    "\n",
    "By definition, the white noise series $w_t$ has $\\mathrm{E}(w_t)=0$ and\n",
    "\n",
    "$\n",
    "\\gamma_w(s,t) = \\operatorname{cov}(w_s, w_t) = \n",
    "\\begin{cases}\n",
    "\\sigma_w^{2} & s = t, \\\\\n",
    "0 & s \\neq t.\n",
    "\\end{cases} \\quad (1.12)\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance of Linear Combination\n",
    "\n",
    "**Property 1.1**: **Covariance of Linear Combinations**\n",
    "\n",
    "If the random variables\n",
    "\n",
    "$\n",
    "U = \\sum_{j=1}^{m} a_j X_j \\quad \\text{and} \\quad V = \\sum_{k=1}^{r} b_k Y_k\n",
    "$\n",
    "\n",
    "are linear combinations of (finite variance) random variables $\\{X_j\\}$ and $\\{Y_k\\}$, respectively, then\n",
    "\n",
    "$\n",
    "\\operatorname{cov}(U, V) = \\sum_{j=1}^{m} \\sum_{k=1}^{r} a_j b_k \\operatorname{cov}(X_j, Y_k). \\quad (1.13)\n",
    "$\n",
    "\n",
    "Furthermore, $\\operatorname{var}(U) = \\operatorname{cov}(U, U)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.18: Autocovariance of a Moving Average\n",
    "\n",
    "$\\gamma_{v}(s,t) = \\operatorname{cov}(v_{s}, v_{t}) = \\operatorname{cov}\\left\\{ \\frac{1}{3}\\left(w_{s-1} + w_{s} + w_{s+1}\\right), \\frac{1}{3}\\left(w_{t-1} + w_{t} + w_{t+1}\\right)\\right\\}.$\n",
    "\n",
    "Noting that $\\operatorname{cov}(w_{s}, w_{t}) = 0$ for $s \\neq t$. \n",
    "\n",
    "When $s = t$ we have:\n",
    "\n",
    "$\\begin{aligned}\n",
    "\\gamma_{v}(t,t) \n",
    "&= \\frac{1}{9}\\operatorname{cov}\\{(w_{t-1} + w_{t} + w_{t+1}), (w_{t-1} + w_{t} + w_{t+1})\\} \\\\\n",
    "&= \\frac{1}{9}[\\operatorname{cov}(w_{t-1}, w_{t-1}) + \\operatorname{cov}(w_{t}, w_{t}) + \\operatorname{cov}(w_{t+1}, w_{t+1}) \\\\\n",
    "&+ \\operatorname{cov}(w_{t-1}, w_{t}) + \\operatorname{cov}(w_{t-1}, w_{t+1}) + \\operatorname{cov}(w_{t+1}, w_{t}) \\\\\n",
    "&+ \\operatorname{cov}(w_{t+1}, w_{t-1}) + \\operatorname{cov}(w_{t}, w_{t-1}) + \\operatorname{cov}(w_{t}, w_{t+1})] \\\\\n",
    "&= \\frac{1}{9}[\\operatorname{cov}(w_{t-1}, w_{t-1}) + \\operatorname{cov}(w_{t}, w_{t}) + \\operatorname{cov}(w_{t+1}, w_{t+1})] \\\\\n",
    "&= \\frac{3}{9}\\sigma_{w}^{2}=\\frac{1}{3}\\sigma_{w}^{2}\n",
    "\\end{aligned}$\n",
    "\n",
    "When $s=t+1$ we have:\n",
    "\n",
    "$\\begin{aligned}\n",
    "\\gamma_v(t+1,t) \n",
    "&= \\frac{1}{9}\\text{cov}\\{(w_t + w_{t+1} + w_{t+2}), (w_{t-1} + w_t + w_{t+1})\\}\\\\\n",
    "&= \\frac{1}{9}[\\text{cov}(w_t, w_t) + \\text{cov}(w_{t+1}, w_{t+1})]\\\\\n",
    "&= \\frac{2}{9}\\sigma_w^2\n",
    "\\end{aligned}$\n",
    "\n",
    "Example 1.18 shows clearly that the smoothing operation introduces a covariance function that decreases as the separation between the two time points increases and disappears completely when the time points are separated by three or more time points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.19: Autocovariance of a Random Walk"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
