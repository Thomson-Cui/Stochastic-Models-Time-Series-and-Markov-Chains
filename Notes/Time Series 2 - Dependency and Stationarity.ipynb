{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency\n",
    "\n",
    "Suppose we observe a time series $\\{x_t$$ at times $t_1, \\ldots, t_n$. The distribution of the observed data $x_{t_1}, \\ldots, x_{t_n}$ is characterized by the joint distribution function:\n",
    "\n",
    "$\n",
    "F_{t_1, \\ldots, t_n}(c_1, \\ldots, c_n) = P(x_{t_1} \\leq c_1, \\ldots, x_{t_n} \\leq c_n)\n",
    "$\n",
    "\n",
    "Usually, this is difficult to work with. We may consider the marginal distribution at time $t$:\n",
    "\n",
    "$\n",
    "F_t(x) = P(x_t \\leq x)\n",
    "$\n",
    "\n",
    "or the marginal density:\n",
    "\n",
    "$\n",
    "f_t(x) = \\frac{\\partial F_t(x)}{\\partial x}\n",
    "$\n",
    "\n",
    "assuming this quantity exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Mean Function\n",
    "\n",
    "The **mean function** of a time series is:\n",
    "\n",
    "$\n",
    "\\mu_{x_t} = E(x_t)\n",
    "$\n",
    "\n",
    "assuming this expectation exists. Note that the mean is a function of $ t $. When there is no ambiguity about which time series we are referring to, we may instead write $\\mu_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.14 Mean Function of a Moving Average Series\n",
    "\n",
    "$w_t$ denotes a white noise series.\n",
    "- $\\mu_{wt}=E(w_t)=0$\n",
    "\n",
    "$\\mu_{vt}=E(v_t)=\\frac{1}{3}[E(w_{t-1})+E(w_{t})+E(w_{t+1})]=0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.15 Mean Function of a Random Walk with Drift\n",
    "\n",
    "$x_{t} = \\delta\\,t + \\sum_{j=1}^{t}w_{j}, \\qquad t=1,2,\\ldots$\n",
    "- $E(w_t)=0$ for all $t$, and $\\delta$ is a constant.\n",
    "\n",
    "$\\mu_{xt} = \\mathrm{E}(x_{t}) = \\delta\\,t + \\sum_{j=1}^{t}\\mathrm{E}(w_{j}) = \\delta\\,t$\n",
    "\n",
    "which is a straight line with slope $\\delta$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.16: Mean Function of Signal Plus Noise\n",
    "\n",
    "$\\begin{aligned}\n",
    "\\mu_{xt} = \\mathrm{E}(x_{t}) \n",
    "&= \\mathrm{E}\\left[2\\cos\\left(2\\pi\\frac{t+15}{50}\\right) + w_{t}\\right] \\\\\n",
    "&= 2\\cos\\left(2\\pi\\frac{t+15}{50}\\right) + \\mathrm{E}(w_{t}) \\\\\n",
    "&= 2\\cos\\left(2\\pi\\frac{t+15}{50}\\right)\n",
    "\\end{aligned}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocovariance Function\n",
    "\n",
    "**Definition 1.2**: The autocovariance function is defined as the product moment\n",
    "\n",
    "$\n",
    "\\gamma_{x}(s,t) = \\operatorname{cov}(x_{s}, x_{t}) = \\operatorname{E}[(x_{s} - \\mu_{s})(x_{t} - \\mu_{t})], \\quad (1.10)\n",
    "$\n",
    "\n",
    "for all $s$ and $t$. When no possible confusion exists about which time series we are referring to, we will drop the subscript and write $\\gamma_{x}(s,t)$ as $\\gamma(s,t)$. Note that $\\gamma_{x}(s,t) = \\gamma_{x}(t,s)$ for all time points $s$ and $t$.\n",
    "\n",
    "- The autocovariance function measures linear dependence in time.\n",
    "- If $x_s$ and $x_t$ are jointly Normally distribution, $\\gamma(s,t)=0$ implies independence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.17 Autocovariance of White Noise\n",
    "\n",
    "By definition, the white noise series $w_t$ has $\\mathrm{E}(w_t)=0$ and\n",
    "\n",
    "$\n",
    "\\gamma_w(s,t) = \\operatorname{cov}(w_s, w_t) = \n",
    "\\begin{cases}\n",
    "\\sigma_w^{2} & s = t, \\\\\n",
    "0 & s \\neq t.\n",
    "\\end{cases} \\quad (1.12)\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance of Linear Combination\n",
    "\n",
    "**Property 1.1**: **Covariance of Linear Combinations**\n",
    "\n",
    "If the random variables\n",
    "\n",
    "$\n",
    "U = \\sum_{j=1}^{m} a_j X_j \\quad \\text{and} \\quad V = \\sum_{k=1}^{r} b_k Y_k\n",
    "$\n",
    "\n",
    "are linear combinations of (finite variance) random variables $\\{X_j\\}$ and $\\{Y_k\\}$, respectively, then\n",
    "\n",
    "$\n",
    "\\operatorname{cov}(U, V) = \\sum_{j=1}^{m} \\sum_{k=1}^{r} a_j b_k \\operatorname{cov}(X_j, Y_k). \\quad (1.13)\n",
    "$\n",
    "\n",
    "Furthermore, $\\operatorname{var}(U) = \\operatorname{cov}(U, U)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.18: Autocovariance of a Moving Average\n",
    "\n",
    "$\\gamma_{v}(s,t) = \\operatorname{cov}(v_{s}, v_{t}) = \\operatorname{cov}\\left\\{ \\frac{1}{3}\\left(w_{s-1} + w_{s} + w_{s+1}\\right), \\frac{1}{3}\\left(w_{t-1} + w_{t} + w_{t+1}\\right)\\right\\}.$\n",
    "\n",
    "Noting that $\\operatorname{cov}(w_{s}, w_{t}) = 0$ for $s \\neq t$. \n",
    "\n",
    "When $s = t$ we have:\n",
    "\n",
    "$\\begin{aligned}\n",
    "\\gamma_{v}(t,t) \n",
    "&= \\frac{1}{9}\\operatorname{cov}\\{(w_{t-1} + w_{t} + w_{t+1}), (w_{t-1} + w_{t} + w_{t+1})\\} \\\\\n",
    "&= \\frac{1}{9}[\\operatorname{cov}(w_{t-1}, w_{t-1}) + \\operatorname{cov}(w_{t}, w_{t}) + \\operatorname{cov}(w_{t+1}, w_{t+1}) \\\\\n",
    "&+ \\operatorname{cov}(w_{t-1}, w_{t}) + \\operatorname{cov}(w_{t-1}, w_{t+1}) + \\operatorname{cov}(w_{t+1}, w_{t}) \\\\\n",
    "&+ \\operatorname{cov}(w_{t+1}, w_{t-1}) + \\operatorname{cov}(w_{t}, w_{t-1}) + \\operatorname{cov}(w_{t}, w_{t+1})] \\\\\n",
    "&= \\frac{1}{9}[\\operatorname{cov}(w_{t-1}, w_{t-1}) + \\operatorname{cov}(w_{t}, w_{t}) + \\operatorname{cov}(w_{t+1}, w_{t+1})] \\\\\n",
    "&= \\frac{3}{9}\\sigma_{w}^{2}=\\frac{1}{3}\\sigma_{w}^{2}\n",
    "\\end{aligned}$\n",
    "\n",
    "When $s=t+1$ we have:\n",
    "\n",
    "$\\begin{aligned}\n",
    "\\gamma_v(t+1,t) \n",
    "&= \\frac{1}{9}\\text{cov}\\{(w_t + w_{t+1} + w_{t+2}), (w_{t-1} + w_t + w_{t+1})\\}\\\\\n",
    "&= \\frac{1}{9}[\\text{cov}(w_t, w_t) + \\text{cov}(w_{t+1}, w_{t+1})]\\\\\n",
    "&= \\frac{2}{9}\\sigma_w^2\n",
    "\\end{aligned}$\n",
    "\n",
    "Example 1.18 shows clearly that the smoothing operation introduces a covariance function that decreases as the separation between the two time points increases and disappears completely when the time points are separated by three or more time points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.19: Autocovariance of a Random Walk\n",
    "\n",
    "For the random walk model, $x_t = \\sum_{j=1}^t w_j$, we have:\n",
    "\n",
    "$\\gamma_x(s,t) = \\text{cov}(x_s, x_t) = \\text{cov}\\left(\\sum_{j=1}^s w_j, \\sum_{k=1}^t w_k\\right) = \\min\\{s,t$ \\sigma_w^2,$\n",
    "\n",
    "For example, with $s = 2$ and $t = 4$:\n",
    "\n",
    "$\\text{cov}(x_2, x_4) = \\text{cov}(w_1 + w_2, w_1 + w_2 + w_3 + w_4) = 2\\sigma_w^2$\n",
    "\n",
    "The variance of the random walk, $\\text{var}(x_t) = \\gamma_x(t,t) = t \\sigma_w^2$, increases without bound as time $t$ increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation Function\n",
    "\n",
    "We can normalize the autocovariance function in the usual way to obtain the **autocorrelation function**:\n",
    "\n",
    "$\n",
    "\\rho(s, t) = \\frac{\\gamma(s, t)}{\\sqrt{\\gamma(s, s) \\gamma(t, t)}}\n",
    "$\n",
    "\n",
    "- $ -1 \\leq \\rho(s, t) \\leq 1 $\n",
    "- Size and sign measure strength and direction of linear relationship\n",
    "- Measure of how well we can forecast with a linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bivariate Time Series\n",
    "\n",
    "Suppose we have two time series $\\{x_t\\}$ and $\\{y_t\\}$ and we consider the problem of predicting $\\{y_t\\}$ using $\\{x_t\\}$. We can consider the **cross-covariance function**:\n",
    "\n",
    "$\n",
    "\\gamma_{xy}(s, t) = \\text{Cov}(x_s, y_t) = E[(x_s - \\mu_{x_s})(y_t - \\mu_{y_t})]\n",
    "$\n",
    "\n",
    "and the **cross-correlation function**:\n",
    "\n",
    "$\n",
    "\\rho_{xy}(s, t) = \\frac{\\gamma_{xy}(s, t)}{\\sqrt{\\gamma_x(s, s) \\gamma_y(t, t)}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationarity\n",
    "\n",
    "In estimation problems, we often consider data consisting of multiple observations from a model, e.g.:\n",
    "\n",
    "$\n",
    "X_1, \\ldots, X_n \\sim \\text{iid } F\n",
    "$\n",
    "\n",
    "However, in time series, our data $ x_{t_1}, \\ldots, x_{t_n} $ often consists of a single observation from a model.\n",
    "\n",
    "To perform estimation and inference, we might ask:\n",
    "\n",
    "- Does $\\{x_t\\}$ exhibit some behavior independent of the absolute point in time?  \n",
    "- Does $\\{x_t\\}$ have some nice long-run time behavior?  \n",
    "\n",
    "If so, then we can draw stronger conclusions about the model from observing many time points, rather than needing many realizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A time series $x_t$ is **strictly stationary** if for all $k = 1, 2, \\ldots$, all choices of time points $t_1, \\ldots, t_k$, and all time shifts $h = \\pm 0, \\pm 1, \\ldots$,\n",
    "\n",
    "$\n",
    "(x_{t_1}, \\ldots, x_{t_k}) \\stackrel{d}{=} (x_{t_1 + h}, \\ldots, x_{t_k + h})\n",
    "$\n",
    "\n",
    "where $\\stackrel{d}{=}$ denotes equality in distribution.\n",
    "\n",
    "If a time series is strictly stationary, then the joint distribution of the observed values depends only on their spacing in time and not on their absolute points in time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition 1.6**: A strictly stationary time series is one for which the probabilistic behavior of every collection of values is identical to that of the time-shifted set; i.e.,\n",
    "\n",
    "$\n",
    "\\{x_{t_{1}},x_{t_{2}},\\ldots,x_{t_{k}}\\} \\stackrel{d}{=} \\{x_{t_{1}+h},x_{t_{2}+h},\\ldots,x_{t_{k}+h}\\}\\quad (1.19)\n",
    "$  \n",
    "\n",
    "\n",
    "for all:\n",
    "- $k = 1,2,...$\n",
    "- Time points $t_{1},t_{2},\\ldots,t_{k}$\n",
    "- Time shifts $h = 0,\\pm 1,\\pm 2,...$\n",
    "\n",
    "where $\\stackrel{d}{=}$ denotes equality in distribution.\n",
    "\n",
    "#### Implications of Strict Stationarity:\n",
    "1. **Distributional Consistency**: All multivariate distributions must match their shifted counterparts for any shift $h$.\n",
    "2. **Single Point Case** ($k=1$):\n",
    "   $\n",
    "   x_{s} \\stackrel{d}{=} x_{t} \\quad \\text{(1.20)}\n",
    "   $\n",
    "   - Implies identical probability distributions at all time points\n",
    "   - Example: Probability of negative value at 1am equals that at 10am\n",
    "   - If mean function $\\mu_t$ exists, it must be constant ($\\mu_s = \\mu_t$)\n",
    "   - Counterexample: Random walk with drift violates this (time-dependent mean)\n",
    "\n",
    "3. **Two Point Case** ($k=2$):\n",
    "   $\n",
    "   \\{x_{s}, x_{t}\\} \\stackrel{d}{=} \\{x_{s+h}, x_{t+h}\\} \\quad \\text{(1.21)}\n",
    "   $\n",
    "   - Autocovariance function satisfies:\n",
    "     $\n",
    "     \\gamma(s,t) = \\gamma(s+h,t+h)\n",
    "     $\n",
    "   - Implies autocovariance depends only on time differences $|s-t|$, not absolute times\n",
    "\n",
    "#### Practical Considerations:\n",
    "- Strict stationarity is too strong for most applications\n",
    "- Difficult to verify from a single dataset\n",
    "- Leads to a weaker definition focusing only on first two moments (mean and covariance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weakly stationary time series\n",
    "\n",
    "**Definition 1.7**: A weakly stationary time series, $ x_t $, is a finite variance process satisfying:\n",
    "\n",
    "1. **Constant Mean**:\n",
    "   $\n",
    "   \\mu_t = E(x_t) \\text{ is constant for all } t\n",
    "   $\n",
    "   (The mean does not depend on time)\n",
    "\n",
    "2. **Autocovariance Depends Only on Time Differences**:\n",
    "   $\n",
    "   \\gamma(s,t) = \\text{cov}(x_s, x_t) \\text{ depends only on } |s - t|\n",
    "   $\n",
    "\n",
    "Henceforth, we will use the term *stationary* to mean *weakly stationary*; if a process is stationary in the strict sense, we will use the term *strictly stationary*.\n",
    "\n",
    "Because the mean function, $\\mathrm{E}(x_{t}) = \\mu_{t}$, of a stationary time series is independent of time $t$, we will write:\n",
    "\n",
    "$\n",
    "\\mu_{t} = \\mu \\quad \\text{(1.22)}\n",
    "$\n",
    "\n",
    "Also, because the autocovariance function, $\\gamma(s,t)$, of a stationary time series, $x_{t}$, depends on $s$ and $t$ only through their difference $|s-t|$, we may simplify the notation. Let $s = t + h$, where $h$ represents the time shift or *lag*. Then:\n",
    "\n",
    "$\n",
    "\\gamma(t+h,t) = \\operatorname{cov}(x_{t+h},x_{t}) = \\operatorname{cov}(x_{h},x_{0}) = \\gamma(h,0)\n",
    "$\n",
    "\n",
    "because the time difference between times $t + h$ and $t$ is the same as the time difference between times $h$ and $0$. Thus, the autocovariance function of a stationary time series does not depend on the time argument $t$. Henceforth, for convenience, we will drop the second argument of $\\gamma(h,0)$ as $\\gamma(h)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition 1.8**: The autocovariance function of a stationary time series is defined as:\n",
    "\n",
    "$\n",
    "\\gamma(h) = \\operatorname{cov}(x_{t+h}, x_t) = \\operatorname{E}[(x_{t+h} - \\mu)(x_t - \\mu)] \\quad \\text{(1.23)}\n",
    "$\n",
    "\n",
    "**Definition 1.9**: The **autocorrelation function** (ACF) of a stationary time series is defined as:\n",
    "\n",
    "$\n",
    "\\rho(h) = \\frac{\\gamma(t+h,t)}{\\sqrt{\\gamma(t+h,t+h)\\gamma(t,t)}} = \\frac{\\gamma(h)}{\\gamma(0)} \\quad \\text{(1.24)}\n",
    "$\n",
    "\n",
    "**Properties**:\n",
    "- By the Cauchy-Schwarz inequality:  \n",
    "  $\n",
    "  -1 \\leq \\rho(h) \\leq 1 \\quad \\text{for all } h\n",
    "  $\n",
    "- Enabling one to assess the relative importance of a given autocorrelation value by comparing with the extreme values $-1$ and $1$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
