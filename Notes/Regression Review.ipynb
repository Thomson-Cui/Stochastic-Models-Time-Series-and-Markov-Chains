{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Review\n",
    "\n",
    "## Simple Linear Regression Model\n",
    "\n",
    "Suppose we have a single response variable $ Y $ and a single predictor variable $ X $. The **simple linear regression model** characterizes the relationship between $ X $ and $ Y $ by\n",
    "\n",
    "$\n",
    "E(Y|X) = \\beta_0 + \\beta_1 X\n",
    "$\n",
    "\n",
    "$\n",
    "Var(Y|X) = \\sigma^2\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have observed data $(x_1, y_1), \\ldots, (x_n, y_n)$. Another way of stating this model is\n",
    "\n",
    "$\n",
    "y_i = \\beta_0 + \\beta_1 x_i + e_i\n",
    "$\n",
    "\n",
    "where the $e_i$ are iid random variables with $E(e_i) = 0$ and $\\text{Var}(e_i) = \\sigma^2$.\n",
    "\n",
    "- $ \\beta_0 $ is the **intercept**, representing the predicted value of $ y $ when $ x = 0 $.  \n",
    "- $ \\beta_1 $ is the **regression coefficient**, indicating the expected change in $ y $ for a one-unit increase in $ x $.  \n",
    "- $ e_i $ is the **error term** (also called the residual), which accounts for the difference between the observed value $ y_i $ and the predicted value $ \\beta_0 + \\beta_1 x_i $. The error terms satisfy:\n",
    "  - $ e_i $ are **independent and identically distributed (iid)** random variables.  \n",
    "  - $ E(e_i) = 0 $: The mean of the errors is 0, meaning the model does not systematically over- or under-predict.  \n",
    "  - $ \\text{Var}(e_i) = \\sigma^2 $: The errors have constant variance, indicating **homoscedasticity** (equal spread of residuals across all $ x $ values).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see these are equivalent by computing\n",
    "\n",
    "$\n",
    "E(Y|X = x_i) = \\beta_0 + \\beta_1 x_i\n",
    "$\n",
    "\n",
    "$\n",
    "\\text{Var}(Y|X = x_i) = \\sigma^2\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Error Term\n",
    "\n",
    "The error term $ e_i $ is the true value of $ y_i $ minus the expected value of $ y_i $:\n",
    "\n",
    "$\n",
    "e_i = y_i - E(Y|X = x_i) = y_i - (\\beta_0 + \\beta_1 x_i)\n",
    "$\n",
    "\n",
    "We make two further assumptions about the $ e_i $:\n",
    "\n",
    "1. $ E(e_i|X = x_i) = 0 $. The mean of $ e_i $ is 0 for every possible $ x_i $.\n",
    "\n",
    "2. The $ e_i $ form an independent collection.\n",
    "\n",
    "Common stronger assumptions include that the $ e_i $ are independent of the $ x_i $ (replacing 1.) and that the $ e_i $ are Normally distributed. Stronger assumptions are needed for things like tests and confidence intervals, but not to derive the basic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation\n",
    "\n",
    "**The goal of simple linear regression** is to estimate the model parameters $\\beta_0$, $\\beta_1$, and $\\sigma^2$. Estimators are denoted with hats, in this case $\\hat{\\beta}_0$, $\\hat{\\beta}_1$, and $\\hat{\\sigma}^2$.\n",
    "\n",
    "We will choose $\\hat{\\beta}_0$, $\\hat{\\beta}_1$, and $\\hat{\\sigma}^2$ to best “fit” the observed data. There are many ways to measure “fit”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitted values and residuals\n",
    "\n",
    "For estimated parameters $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$, we define:\n",
    "\n",
    "1. The **fitted values** $\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i$.\n",
    "\n",
    "2. The **residuals** $\\hat{e}_i = y_i - \\hat{y}_i$. \n",
    "\n",
    "    These are the plug-in estimators for $y_i$ and $e_i$ using the estimated values $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares Estimation\n",
    "\n",
    "One way to choose $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ is with the **least squares criterion**, leading to **Ordinary Least Squares (OLS)** regression. The least squares criterion says to choose $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ to be the values of $\\beta_0$ and $\\beta_1$ that minimize\n",
    "\n",
    "$\n",
    "RSS = \\sum_{i=1}^{n} \\left( y_i - (\\beta_0 + \\beta_1 x_i) \\right)^2\n",
    "$\n",
    "\n",
    "$ RSS $ stands for **residual sum of squares** and represents the sum of the squared vertical distances from $ y_i $ to the fitted value $ \\hat{y}_i $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deriving OLS Regression\n",
    "\n",
    "We can derive the least squares estimates of $\\beta_0$ and $\\beta_1$ using multivariate calculus. First, we find the critical point where\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\beta_0} \\sum_{i=1}^n \\left( y_i - (\\beta_0 + \\beta_1 x_i) \\right)^2 = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\beta_1} \\sum_{i=1}^n \\left( y_i - (\\beta_0 + \\beta_1 x_i) \\right)^2 = 0\n",
    "$$\n",
    "\n",
    "\n",
    "We then verify that this is the minimum by checking that $|H| > 0$, where $H$ is the Hessian matrix. See Appendix A.3 of the book for the full derivation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulas for OLS Regression\n",
    "\n",
    "The following are formulas for the least squares estimates of $\\beta_0$ and $\\beta_1$:\n",
    "\n",
    "$\n",
    "\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\n",
    "$\n",
    "\n",
    "$\n",
    "\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\n",
    "$\n",
    "\n",
    "While we will use technology to compute these estimates when working with data, these formulas are useful for understanding the properties of the estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting OLS Regression in R\n",
    "\n",
    "We can easily compute $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ in $R$ using the function `lm()`. We can call:\n",
    "\n",
    "$\n",
    "fit = lm(y \\sim x)\n",
    "$\n",
    "$\n",
    "summary(fit)\n",
    "$\n",
    "\n",
    "This reports $\\hat{\\beta}_0$ (the intercept) and $\\hat{\\beta}_1$ (the slope associated with the predictor variable $x$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating $\\sigma^2$\n",
    "\n",
    "We estimate $ \\sigma^2 $ by\n",
    "\n",
    "$\n",
    "\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^{n} e_i^2}{n-2}\n",
    "$\n",
    "\n",
    "The $ n-2 $ is because we lose 2 degrees of freedom from estimating $ \\hat{\\beta}_0 $ and $ \\hat{\\beta}_1 $. $ \\hat{\\sigma} = \\sqrt{\\hat{\\sigma}^2} $ is called the **residual standard error**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of OLS Estimators\n",
    "\n",
    "1. Under our regression assumptions $E(\\hat\\beta_1|X)=\\beta_1$\n",
    "\n",
    "    $\\begin{aligned}\n",
    "    E(\\hat{\\beta}_1|X) &= E\\left(\\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\\right) \\\\\n",
    "    &= \\frac{1}{\\sum_{i=1}^n (x_i - \\bar{x})^2} E\\left(\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})\\right) \\\\\n",
    "    &= \\frac{1}{\\sum_{i=1}^n (x_i - \\bar{x})^2} E\\left(\\sum_{i=1}^n (x_i - \\bar{x})(\\beta_0 + \\beta_1 x_i + e_i - (\\beta_0 + \\beta_1 \\bar{x} + \\bar{e}))\\right) \\\\\n",
    "    &= \\frac{1}{\\sum_{i=1}^n (x_i - \\bar{x})^2} E\\left(\\sum_{i=1}^n (x_i - \\bar{x})(\\beta_1(x_i - \\bar{x}) + e_i - \\bar{e})\\right) \\\\\n",
    "    &= \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2} E(\\beta_1) + E\\left(\\sum_{i=1}^n (x_i - \\bar{x})(e_i - \\bar{e})\\right) \\\\\n",
    "    &= \\beta_1 + 0 = \\beta_1\n",
    "    \\end{aligned}$\n",
    "2. Under our regression assumptions $E(\\hat\\beta_0|X)=\\beta_0$\n",
    "\n",
    "    $\\begin{aligned}\n",
    "    E(\\hat{\\beta}_0|X) &= E(\\bar{y} - \\hat{\\beta}_1 \\bar{x}) \\\\\n",
    "    &= \\bar{y} - \\bar{x} E(\\hat{\\beta}_1) \\\\\n",
    "    &= \\bar{y} - \\bar{x} \\beta_1 \\quad \\text{since } \\hat{\\beta}_1\\text{ is unbiased} \\\\\n",
    "    &= \\beta_0\n",
    "    \\end{aligned}$\n",
    "3. Under our regression assumptions $E(\\hat\\sigma^2|X)=\\hat\\sigma^2$.\n",
    "\n",
    "    The proof is beyond the scope of our class and involves $\\chi^2$ distributions.\n",
    "4. Under our regression assumptions\n",
    "    - $ Var(\\hat{\\beta}_1 | X) = \\sigma^2 \\frac{1}{\\sum_{i=1}^n (x_i - \\bar{x})^2} $.\n",
    "    - $ Var(\\hat{\\beta}_0 | X) = \\sigma^2 \\left( \\frac{1}{n} + \\frac{\\bar{x}}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\right) $.\n",
    "    - $ Cov(\\hat{\\beta}_1, \\hat{\\beta}_0 | X) = -\\sigma^2 \\frac{\\bar{x}}{\\sum_{i=1}^n (x_i - \\bar{x})^2} $.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Errors\n",
    "\n",
    "We may wish to estimate the variances or standard errors of $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$.  \n",
    "We substitute $\\hat{\\sigma}^2$ for $\\sigma^2$ and obtain the plug-in estimators:\n",
    "\n",
    "- $ Var(\\hat{\\beta}_1|X) = \\hat{\\sigma}^2 \\frac{1}{\\sum_{i=1}^{n}(x_i-\\bar{x})^2} $.  \n",
    "- $ se(\\hat{\\beta}_1|X) = \\sqrt{Var(\\hat{\\beta}_1|X)} $.  \n",
    "- $ Var(\\hat{\\beta}_0|X) = \\hat{\\sigma}^2 \\left( \\frac{1}{n} + \\frac{\\bar{x}}{\\sum_{i=1}^{n}(x_i-\\bar{x})^2} \\right) $.  \n",
    "- $ se(\\hat{\\beta}_0|X) = \\sqrt{Var(\\hat{\\beta}_0|X)} $.  \n",
    "\n",
    "These standard errors are output as part of `summary(fit)` in R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Errors\n",
    "\n",
    "If we assume the $ e_i $ are iid Normal random variables with mean 0 and variance $ \\sigma^2 $, we can develop tests and confidence intervals for $ \\hat{\\beta}_0 $ and $ \\hat{\\beta}_1 $.\n",
    "\n",
    "In this case, $ \\hat{\\beta}_0 $ and $ \\hat{\\beta}_1 $ are also Normally distributed. (Why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $t$ Tests and Confidence Intervals\n",
    "\n",
    "Recall that if a random variable $U$ follows a Normal distribution then\n",
    "\n",
    "$\\frac{U-\\mu_u}{\\hat\\sigma_u}\\sim t_{df}$\n",
    "\n",
    "where $df$ is the degrees of freedom of $\\hat\\sigma_u$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the fact that $\\hat{\\sigma}^2$ has $n-2$ degrees of freedom, we can conclude:\n",
    "\n",
    "$\n",
    "\\frac{\\hat{\\beta}_1 - \\beta_1}{\\text{se}(\\hat{\\beta}_1|X)} \\sim t_{n-2}\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\hat{\\beta}_0 - \\beta_0}{\\text{se}(\\hat{\\beta}_0|X)} \\sim t_{n-2}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, to test the hypotheses:\n",
    "\n",
    "$\n",
    "H_0 : \\beta_1 = \\beta_1^*\n",
    "$\n",
    "$\n",
    "H_1 : \\beta_1 \\neq \\beta_1^*\n",
    "$\n",
    "\n",
    "we obtain the test statistic $ t^* = \\frac{\\hat{\\beta}_1 - \\beta_1}{se(\\hat{\\beta}_1 | X)} $ and the p-value $ P(t_{n-2} > | t^* |) $.\n",
    "\n",
    "To test the hypotheses:\n",
    "\n",
    "$\n",
    "H_0 : \\beta_0 = \\beta_0^*\n",
    "$\n",
    "$\n",
    "H_1 : \\beta_0 \\neq \\beta_0^*\n",
    "$\n",
    "\n",
    "we obtain the test statistic $ t^* = \\frac{\\hat{\\beta}_0 - \\beta_0}{se(\\hat{\\beta}_0 | X)} $ and the p-value $ P(t_{n-2} > | t^* |) $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`summary(fit)` in R reports $|t^*|$ and the p-value $P(t_{n-2} > |t^*|)$ for the tests:\n",
    "\n",
    "$\n",
    "H_0 : \\beta_1 = 0\n",
    "$\n",
    "$\n",
    "H_1 : \\beta_1 \\neq 0\n",
    "$\n",
    "\n",
    "and\n",
    "\n",
    "$\n",
    "H_0 : \\beta_0 = 0\n",
    "$\n",
    "$\n",
    "H_1 : \\beta_0 \\neq 0\n",
    "$\n",
    "\n",
    "In particular, the p-value from the first test is often used as a measure of evidence that $X$ is linearly associated with (and thus useful for predicting) $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100(1 - $\\alpha$)% confidence intervals can be obtained by:\n",
    "\n",
    "$\n",
    "\\hat{\\beta}_1 \\pm t_{1-\\alpha/2,n-2} \\cdot se(\\hat{\\beta}_1)\n",
    "$\n",
    "\n",
    "and\n",
    "\n",
    "$\n",
    "\\hat{\\beta}_0 \\pm t_{1-\\alpha/2,n-2} \\cdot se(\\hat{\\beta}_0)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "An important use of regression models is predicting the value of the response $ y^* $ for a new value of the predictor $ x^* $. We can observe:\n",
    "\n",
    "$\n",
    "y^* = \\beta_0 + \\beta_1 x^* + e^*\n",
    "$\n",
    "\n",
    "Thus, $ \\hat{y}^* = \\hat{\\beta}_0 + \\hat{\\beta}_1 x^* $ is an unbiased estimator of $ y^* $ since:\n",
    "\n",
    "$\n",
    "E(\\hat{y}^*|x^*) = \\beta_0 + \\beta_1 x^* = y^*\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also wish to quantify the uncertainty associated with our prediction.\n",
    "\n",
    "$\n",
    "Var(\\hat{y}^*|x^*) = \\sigma^2 \\left( 1 + \\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2} \\right)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Intervals\n",
    "\n",
    "If the $ e_i $ are Normally distributed, then so is $ \\tilde{y}^* $ (Why?). A confidence interval for $ y^* $ is called a **prediction interval**. The 100(1 - $ \\alpha $)% prediction interval for $ y^* $ given $ x^* $ is:\n",
    "\n",
    "$\n",
    "\\tilde{y}^* \\pm t_{1 - \\alpha/2, n - 2} \\cdot \\text{se}(y^* | x^*)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction in R\n",
    "\n",
    "Prediction in R can be performed for a wide variety of models using the powerful `predict()` function. Suppose we have fit a model (for example using `fit = lm(y ~ x)`). Let `newx` be a data frame containing the $ x^* $ where we want predictions of $ y^* $. We can obtain these by:\n",
    "\n",
    "**predict(fit, newx)**\n",
    "\n",
    "We can obtain prediction intervals using:\n",
    "\n",
    "**predict(fit, newdata, interval = \"prediction\")**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Intervals for Fitted Values\n",
    "\n",
    "We can also express our uncertainty about our estimate $\\hat{y}$ of $E(Y|x)$.  \n",
    "We can observe that:\n",
    "\n",
    "$\n",
    "Var(\\hat{y}|x) = \\sigma^2 \\left( \\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\right)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could obtain a confidence interval for a particular $ x = x_i $ using a $ t $ interval, but it is more common to create a confidence band for all $ x $ values simultaneously. The 100(1 - $ \\alpha $)% confidence band is:\n",
    "\n",
    "$\n",
    "(\\hat{\\beta}_0 + \\hat{\\beta}_1 x) \\pm \\left(2F_{\\alpha, 2, n-2}\\right)^2 se(\\hat{y}|x)\n",
    "$\n",
    "\n",
    "$ F_{2, n-2} $ is an $ F $ distribution with 2 and $ n-2 $ degrees of freedom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Bands in R\n",
    "\n",
    "We can obtain confidence bands in R by defining a data frame `grid` containing a fine grid of $ x $ values and using:\n",
    "\n",
    "```R\n",
    "predict(fit, grid, interval = \"confidence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Coefficient of Determination\n",
    "\n",
    "The OLS line is the “best” fit in some sense, but how good is it? One measure is the coefficient of determination $ R^2 $. It is defined by:\n",
    "\n",
    "$\n",
    "R^2 = 1 - \\frac{\\sum_{i=1}^n \\hat{e}_i^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}\n",
    "$\n",
    "\n",
    "$ R^2 $ measures the proportion of variation in $ y $ explained by the model using $ x $. $\\sum_{i=1}^n (y_i - \\bar{y})^2$ is the total variation in $ y $, while $\\sum_{i=1}^n \\hat{e}_i^2$ is the remaining variation after fitting the OLS model using $ x $. This is the “Multiple R-Squared” reported in `summary(fit)` in $ R $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation\n",
    "\n",
    "The correlation between $ x $ and $ y $, written $ r_{xy} $, is a measure of the strength and direction of the linear relationship between $ x $ and $ y $. It is related to $ R^2 $ by:\n",
    "\n",
    "$\n",
    "r_{xy} = \\sqrt{R^2}\n",
    "$\n",
    "\n",
    "with the sign of the square root being determined by the sign of $ \\hat{\\beta}_1 $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residuals\n",
    "\n",
    "The residuals (the $\\hat{e}_i$) are useful for checking our assumptions about the $e_i$, such as whether the $e_i$ are mean 0, constant variance, or Normally distributed. There are many ways one can use the residuals, some of which we will touch on throughout this class. One of the most basic is plotting the residuals vs the $x$ or the fitted values."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
